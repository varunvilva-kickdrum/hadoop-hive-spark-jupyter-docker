{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c132e650-e5fe-4a52-a14a-2e004daddaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c43889777603:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HudiTest</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7aca7825df50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, current_date, col\n",
    "\n",
    "# Create Spark session with Hudi configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HudiTest\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hudi-spark3-bundle_2.12-1.0.0.jar\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\") \\\n",
    "    .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\") \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea48a8a-020b-4667-9fa5-6fc8f67cf04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|      date|           timestamp|\n",
      "+---+----------+--------------------+\n",
      "|  0|2025-05-22|2025-05-22 17:19:...|\n",
      "|  1|2025-05-22|2025-05-22 17:19:...|\n",
      "|  2|2025-05-22|2025-05-22 17:19:...|\n",
      "|  3|2025-05-22|2025-05-22 17:19:...|\n",
      "|  4|2025-05-22|2025-05-22 17:19:...|\n",
      "|  5|2025-05-22|2025-05-22 17:19:...|\n",
      "|  6|2025-05-22|2025-05-22 17:19:...|\n",
      "|  7|2025-05-22|2025-05-22 17:19:...|\n",
      "|  8|2025-05-22|2025-05-22 17:19:...|\n",
      "|  9|2025-05-22|2025-05-22 17:19:...|\n",
      "| 10|2025-05-22|2025-05-22 17:19:...|\n",
      "| 11|2025-05-22|2025-05-22 17:19:...|\n",
      "| 12|2025-05-22|2025-05-22 17:19:...|\n",
      "| 13|2025-05-22|2025-05-22 17:19:...|\n",
      "| 14|2025-05-22|2025-05-22 17:19:...|\n",
      "| 15|2025-05-22|2025-05-22 17:19:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(0, 16).withColumn(\"date\", current_date()).withColumn(\"timestamp\", current_timestamp())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c942da42-a983-4a84-96ae-f9efa1e835dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_options = {\n",
    "    'hoodie.table.name': 'my_table',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',                           # unique identifier for records\n",
    "    'hoodie.datasource.write.partitionpath.field': 'date',                     # partition column (must exist in df)\n",
    "    'hoodie.datasource.write.precombine.field': 'timestamp',                   # used to deduplicate newer rows\n",
    "    'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.SimpleKeyGenerator',\n",
    "    'hoodie.datasource.write.operation': 'upsert',                             # or \"insert\" for first-time load\n",
    "    'hoodie.datasource.write.table.type': 'COPY_ON_WRITE',                     # or MERGE_ON_READ\n",
    "    'hoodie.datasource.hive_sync.enable': 'false',                             # turn off Hive sync for now\n",
    "    'hoodie.datasource.write.hive_style_partitioning': 'true',                 # optional: partitions as date=2024-05-21\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95ec667-2f4b-43c5-8cde-0202b51b3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write data\n",
    "df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_options) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"hdfs://namenode:9000/data/hudi/my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312eb4f0-327e-4cd3-b268-b010c175eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+--------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|      date|           timestamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+--------------------+\n",
      "|  20250522171915221|20250522171915221...|                 7|       date=2025-05-22|179eb18e-41c1-438...|  7|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 3|       date=2025-05-22|179eb18e-41c1-438...|  3|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                14|       date=2025-05-22|179eb18e-41c1-438...| 14|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                10|       date=2025-05-22|179eb18e-41c1-438...| 10|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 9|       date=2025-05-22|179eb18e-41c1-438...|  9|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 5|       date=2025-05-22|179eb18e-41c1-438...|  5|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                12|       date=2025-05-22|179eb18e-41c1-438...| 12|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 1|       date=2025-05-22|179eb18e-41c1-438...|  1|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                15|       date=2025-05-22|179eb18e-41c1-438...| 15|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                11|       date=2025-05-22|179eb18e-41c1-438...| 11|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 4|       date=2025-05-22|179eb18e-41c1-438...|  4|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 8|       date=2025-05-22|179eb18e-41c1-438...|  8|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 0|       date=2025-05-22|179eb18e-41c1-438...|  0|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                13|       date=2025-05-22|179eb18e-41c1-438...| 13|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 6|       date=2025-05-22|179eb18e-41c1-438...|  6|2025-05-22|2025-05-22 17:19:...|\n",
      "|  20250522171915221|20250522171915221...|                 2|       date=2025-05-22|179eb18e-41c1-438...|  2|2025-05-22|2025-05-22 17:19:...|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read = spark.read.format(\"hudi\").load(\"hdfs://namenode:9000/data/hudi/my_table\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a14021-ece4-47c2-83e6-c57fdc2e5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.format(\"hudi\").load(\"hdfs://namenode:9000/data/hudi/ev_data_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b344018-660c-4eab-a163-922e2ad9e4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_hoodie_commit_time: string, _hoodie_commit_seqno: string, _hoodie_record_key: string, _hoodie_partition_path: string, _hoodie_file_name: string, vehical_number: string, county: string, city: string, state: string, postal_code: int, model_year: int, make: string, model: string, vehical_type: string, cavf_eligibility: string, electric_range: int, base_msrp: int, legislative_district: int, electric_utility: string, longitude: double, latitude: double, event_time: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09899fd2-1592-4261-a0a5-4e80224f84ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
